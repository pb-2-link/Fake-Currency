{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from sklearn.model_split import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_paths, target_size=(224, 224)):\n",
    "\n",
    "    processed_images = []\n",
    "    for path in image_paths:\n",
    "        # Read image\n",
    "        img = cv2.imread(path)\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize\n",
    "        resized = cv2.resize(gray, target_size)\n",
    "        # Normalize pixel values\n",
    "        normalized = resized / 255.0\n",
    "        processed_images.append(normalized)\n",
    "    \n",
    "    return np.array(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denomination_model(input_shape, num_classes):\n",
    "\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Fourth convolutional block\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_authenticity_model(input_shape):\n",
    "\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional block with residual connection\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_images, train_denominations, train_authenticity,\n",
    "                val_images, val_denominations, val_authenticity,\n",
    "                input_shape, num_classes, epochs=50, batch_size=32):\n",
    "    denom_model = create_denomination_model(input_shape, num_classes)\n",
    "    denom_history = denom_model.fit(\n",
    "        train_images, train_denominations,\n",
    "        validation_data=(val_images, val_denominations),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create and train authenticity model\n",
    "    auth_model = create_authenticity_model(input_shape)\n",
    "    auth_history = auth_model.fit(\n",
    "        train_images, train_authenticity,\n",
    "        validation_data=(val_images, val_authenticity),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return denom_model, auth_model, denom_history, auth_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_paths = [...] \n",
    "    denomination_labels = [...]  \n",
    "    authenticity_labels = [...]  \n",
    "\n",
    "    processed_images = preprocess_images(image_paths)\n",
    "    \n",
    "\n",
    "    X_train, X_val, y_denom_train, y_denom_val, y_auth_train, y_auth_val = train_test_split(\n",
    "        processed_images, denomination_labels, authenticity_labels,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "\n",
    "    y_denom_train = tf.keras.utils.to_categorical(y_denom_train)\n",
    "    y_denom_val = tf.keras.utils.to_categorical(y_denom_val)\n",
    "    \n",
    "    input_shape = (224, 224, 1) \n",
    "    num_classes = len(set(denomination_labels))\n",
    "    \n",
    "    denom_model, auth_model, denom_history, auth_history = train_models(\n",
    "        X_train, y_denom_train, y_auth_train,\n",
    "        X_val, y_denom_val, y_auth_val,\n",
    "        input_shape, num_classes\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
